{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f15afcf-b8df-4dc4-b5bd-401ca429bdce",
   "metadata": {},
   "source": [
    "# Introduction to OpenSearch\n",
    "\n",
    "A server is available on the cluster for this course. If you really need to you can set up your own server in your local machine. I advise you to use docker: https://opensearch.org/docs/latest/opensearch/install/docker/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74452b2-5e1a-4eb5-9cdd-fd9bc3a3603d",
   "metadata": {},
   "source": [
    "## CURL Connection to server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb245c-8503-4819-a30c-8ac9e9c6f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "import requests\n",
    "\n",
    "host = '10.10.255.202'\n",
    "port = 8200\n",
    "index_name = 'user220'\n",
    "auth = ('user220', 'password here') # For testing only. Don't store credentials in code.\n",
    "\n",
    "s = requests.Session()\n",
    "s.auth = auth\n",
    "\n",
    "#auth = (index_name, 'zya*xJ!4]n') # For testing only. Don't store credentials in code.\n",
    "ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA.\n",
    "server_uri = 'https://' + host + ':' + str(port)\n",
    "\n",
    "# function for the cURL requests\n",
    "def opensearch_curl(uri = '/' , body='', verb='get'):\n",
    "    # pass header option for content type if request has a\n",
    "    # body to avoid Content-Type error in Elasticsearch v6.0\n",
    "    \n",
    "    uri = server_uri + uri\n",
    "    print(uri)\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # make HTTP verb parameter case-insensitive by converting to lower()\n",
    "        if verb.lower() == \"get\":\n",
    "            resp = s.get(uri, json=body, headers=headers, verify=False)\n",
    "        elif verb.lower() == \"post\":\n",
    "            resp = s.post(uri, json=body, headers=headers, verify=False)\n",
    "        elif verb.lower() == \"put\":\n",
    "            resp = s.put(uri, json=body, headers=headers, verify=False)\n",
    "        elif verb.lower() == \"del\":\n",
    "                resp = s.delete(uri, json=body, headers=headers, verify=False)\n",
    "        elif verb.lower() == \"head\":\n",
    "                resp = s.head(uri, json=body, headers=headers, verify=False)\n",
    "\n",
    "        # read the text object string\n",
    "        try:\n",
    "            resp_text = json.loads(resp.text)\n",
    "        except:\n",
    "            resp_text = resp.text\n",
    "\n",
    "        # catch exceptions and print errors to terminal\n",
    "    except Exception as error:\n",
    "        print ('\\nelasticsearch_curl() error:', error)\n",
    "        resp_text = error\n",
    "\n",
    "    # return the Python dict of the request\n",
    "    return resp_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b4b34-d8bb-4915-ba1c-501fbba2c7fc",
   "metadata": {},
   "source": [
    "## OpenSearch Python API \n",
    "\n",
    "A short introduction is available here:\n",
    "https://opensearch.org/docs/latest/clients/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3f988-477d-4cb8-816b-70b230d7a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearchpy import helpers\n",
    "\n",
    "# Optional client certificates if you don't want to use HTTP basic authentication.\n",
    "# client_cert_path = '/full/path/to/client.pem'\n",
    "# client_key_path = '/full/path/to/client-key.pem'\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = auth,\n",
    "    # client_cert = client_cert_path,\n",
    "    # client_key = client_key_path,\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    "    #, ca_certs = ca_certs_path\n",
    ")\n",
    "\n",
    "if client.indices.exists(index_name):\n",
    "\n",
    "    client.indices.open(index = index_name)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX SETTINGS')\n",
    "    index_settings = {\n",
    "        \"settings\":{\n",
    "          \"index\":{\n",
    "             \"refresh_interval\" : \"1s\"\n",
    "          }\n",
    "       }\n",
    "    }\n",
    "    client.indices.put_settings(index = index_name, body = index_settings)\n",
    "    settings = client.indices.get_settings(index = index_name)\n",
    "    pp.pprint(settings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX MAPPINGS')\n",
    "    mappings = client.indices.get_mapping(index = index_name)\n",
    "    pp.pprint(mappings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX #DOCs')\n",
    "    print(client.count(index = index_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74203094-61c8-4f9f-9372-259cba0474a1",
   "metadata": {},
   "source": [
    "# Index creation and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec20622-378e-43a1-8603-02b3eaf1df90",
   "metadata": {},
   "source": [
    "## Create an index with your own settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f19c7-f43d-48ef-a6f2-24d6e1b31d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_body = {\n",
    "   \"settings\":{\n",
    "      \"index\":{\n",
    "         \"number_of_replicas\":0,\n",
    "         \"number_of_shards\":4,\n",
    "         \"refresh_interval\":\"-1\",\n",
    "         \"knn\":\"true\"\n",
    "      },\n",
    "      \"analysis\":{\n",
    "         \"filter\":{\n",
    "            \"edge_ngram_filter\":{\n",
    "               \"type\":\"edge_ngram\",\n",
    "               \"min_gram\":1,\n",
    "               \"max_gram\":20\n",
    "            }\n",
    "         },\n",
    "         \"analyzer\":{\n",
    "            \"my_analyzer\":{\n",
    "               \"type\":\"custom\",\n",
    "               \"tokenizer\":\"standard\",\n",
    "               \"filter\":[\n",
    "                  \"lowercase\",\n",
    "                  \"edge_ngram_filter\"\n",
    "               ]\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "   },\n",
    "   \"mappings\":{\n",
    "      \"properties\":{\n",
    "         \"doc_id\":{\n",
    "            \"type\":\"keyword\"\n",
    "         },\n",
    "         \"contents\":{\n",
    "            \"type\":\"text\",\n",
    "            \"analyzer\": \"standard\",\n",
    "#            \"analyzer\":\"my_analyzer\",\n",
    "            \"similarity\":\"BM25\"\n",
    "         },\n",
    "         \"sentence_embedding\":{\n",
    "            \"type\":\"knn_vector\",\n",
    "            \"dimension\": 768,\n",
    "            \"method\":{\n",
    "               \"name\":\"hnsw\",\n",
    "               \"space_type\":\"innerproduct\",\n",
    "               \"engine\":\"faiss\",\n",
    "               \"parameters\":{\n",
    "                  \"ef_construction\":256,\n",
    "                  \"m\":48\n",
    "               }\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    print(\"Index already existed. Nothing to be done.\")\n",
    "else:        \n",
    "    response = client.indices.create(index_name, body=index_body)\n",
    "    print('\\nCreating index:')\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926fc1ba-4906-460e-9c35-926acf3d24ec",
   "metadata": {},
   "source": [
    "## Check the indexes, settings and mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd4df4-5a77-44fd-b62e-b3dcac013e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n----------------------------------------------------------------------------------- INDEX SETTINGS')\n",
    "index_settings = {\n",
    "    \"settings\":{\n",
    "      \"index\":{\n",
    "         \"refresh_interval\" : \"1s\"\n",
    "      }\n",
    "   }\n",
    "}\n",
    "client.indices.put_settings(index = index_name, body = index_settings)\n",
    "settings = client.indices.get_settings(index = index_name)\n",
    "pp.pprint(settings)\n",
    "\n",
    "print('\\n----------------------------------------------------------------------------------- INDEX MAPPINGS')\n",
    "mappings = client.indices.get_mapping(index = index_name)\n",
    "pp.pprint(mappings)\n",
    "\n",
    "print('\\n----------------------------------------------------------------------------------- INDEX #DOCs')\n",
    "print(client.count(index = index_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c8045-9117-4e45-831d-dd52b4b69581",
   "metadata": {},
   "source": [
    "## Delete the index if you want to replace it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd757fe9-d568-4ed8-84cf-76e568a7e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#be absolutely sure that you want to comment this line and actually delete the index!!!\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    # Delete the index.\n",
    "    response = client.indices.delete(\n",
    "        index = index_name,\n",
    "        timeout = \"600s\"\n",
    "    )\n",
    "    print('\\nDeleting index:')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f11b56-ef9b-44c7-90b9-7e70c077a12e",
   "metadata": {},
   "source": [
    "# Document Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9aa051-889f-40f0-8e4e-4818efe90f2c",
   "metadata": {},
   "source": [
    "## Text tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf0559-1146-4a9b-8529-aebbe60575f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "save_figures = False\n",
    "\n",
    "print(\"token\".ljust(10), \"lemma\".ljust(10), \"pos\".ljust(6), \"tag\".ljust(6), \"dep\".ljust(10),\n",
    "            \"shape\".ljust(10), \"alpha\", \"stop\")\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "for token in doc:\n",
    "    print(token.text.ljust(10), token.lemma_.ljust(10), token.pos_.ljust(6), token.tag_.ljust(6), token.dep_.ljust(10),\n",
    "            token.shape_.ljust(10), token.is_alpha, token.is_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a470b87-5ad3-480e-a225-467f8259fdd1",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45616666-b424-4a0e-9852-498857da5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text.ljust(12), ent.label_.ljust(10), ent.start_char, ent.end_char)\n",
    "\n",
    "html_ent = displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce4e2d-109a-4da5-93da-67ada80fc4d5",
   "metadata": {},
   "source": [
    "## Dual-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d3fdd-1e56-406b-a635-dd45fa22f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take average of all tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-v2\")\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "doc_emb = encode(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a286b-b31b-4211-8dc2-def52a9be649",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1fd78-a559-4e8d-836e-be1553de27be",
   "metadata": {},
   "source": [
    "## Training Dual-Encoders\n",
    "\n",
    "      https://www.sbert.net/docs/training/overview.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b3820-85dd-462a-ab88-7a8ae564c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "#Define your train examples. You need more than just two examples...\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "\n",
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32f953-2533-4e93-a7c0-4a0470e1f32c",
   "metadata": {},
   "source": [
    "# Document indexing\n",
    "\n",
    "## Simple Document indexing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de6117-04a3-4eb0-b182-fcc66fed830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = {\n",
    "    'doc_id': 'documentA',\n",
    "    'contents': docs[0],\n",
    "    'sentence_embedding': doc_emb[0].numpy()\n",
    "}\n",
    "resp = client.index(index=index_name, id=1, body=doc)\n",
    "print(resp['result'])\n",
    "\n",
    "doc = {\n",
    "    'doc_id': 'documentB',\n",
    "    'contents': docs[1],\n",
    "    'sentence_embedding': doc_emb[1].numpy()\n",
    "}\n",
    "resp = client.index(index=index_name, id=2, body=doc)\n",
    "print(resp['result'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c36e16-5fd4-4c6e-806a-30a1bac5ee46",
   "metadata": {},
   "source": [
    "## Deletion of documents and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04018f-2ed8-49c0-a901-d4d9877a8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "Delete the document.\n",
    "response = client.delete(\n",
    "    index = index_name,\n",
    "    id = id\n",
    ")\n",
    "\n",
    "print('\\nDeleting document:')\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046da48-c98e-4ec3-8df8-a5066c9597bc",
   "metadata": {},
   "source": [
    "# Search\n",
    "\n",
    "OpenSearch implements a Query Syntax Language that supports a wide range of search options.\n",
    "\n",
    "     https://opensearch.org/docs/latest/opensearch/query-dsl/index/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db83cf-94fd-4416-8438-3d15f2899928",
   "metadata": {},
   "source": [
    "## Text-based Search\n",
    "\n",
    "The text-based search documentation is available here:\n",
    "\n",
    "     https://opensearch.org/docs/latest/opensearch/query-dsl/full-text/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5beaf96-5c80-4853-a7ac-20784a633f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How many people live in London?\"\n",
    "\n",
    "query_bm25 = {\n",
    "  'size': 5,\n",
    "#  '_source': ['doc_id', 'contents', 'sentence_embedding'],\n",
    "  '_source': ['doc_id', 'contents'],\n",
    "#  '_source': ['doc_id'],\n",
    "#  '_source': '',\n",
    "  'query': {\n",
    "    'match': {\n",
    "      'contents': query,\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body = query_bm25,\n",
    "    index = index_name\n",
    ")\n",
    "\n",
    "print('\\nSearch results:')\n",
    "pp.pprint(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df3cb7-927f-40ac-9b46-a75e4fc9a9c1",
   "metadata": {},
   "source": [
    "## Embedding Spaces Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2f2bf-7513-46cc-8dc1-4e7acbf4bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the query embedding\n",
    "query = \"How many people live in London?\"\n",
    "query_emb = encode(query)\n",
    "\n",
    "query_denc = {\n",
    "  'size': 5,\n",
    "#  '_source': ['doc_id', 'contents', 'sentence_embedding'],\n",
    "#  '_source': ['doc_id', 'contents'],\n",
    "  '_source': ['doc_id'],\n",
    "   \"query\": {\n",
    "        \"knn\": {\n",
    "          \"sentence_embedding\": {\n",
    "            \"vector\": query_emb[0].numpy(),\n",
    "            \"k\": 2\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body = query_denc,\n",
    "    index = index_name\n",
    ")\n",
    "\n",
    "print('\\nSearch results:')\n",
    "pp.pprint(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd4218-1a32-458c-acf9-77fcf1693d4a",
   "metadata": {},
   "source": [
    "# Close the index and refresh it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ca887-56bf-4846-a603-3596114c8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOT SURE IF THIS IS NEEDED WITH FAISS\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\":{\n",
    "      \"index\":{\n",
    "         \"refresh_interval\" : \"1s\"\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "client.indices.close(index = index_name, timeout=\"600s\")\n",
    "client.indices.put_settings(index = index_name, body = index_settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP & IR",
   "language": "python",
   "name": "nlp-ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
