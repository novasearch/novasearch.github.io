{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44650c1c-d142-4fa6-bfe9-783517e0dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import model_view, head_view\n",
    "from transformers import *\n",
    "\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca5aa6f-877d-4af9-8325-202cd9c2a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d07011-a508-4b60-9d52-e7b97511817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b3ad2-0314-42fc-8832-e816125dd66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path,  output_hidden_states=True, output_attentions=True)  \n",
    "model = AutoModel.from_pretrained(model_path, config=config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259f0c2-ab89-43bc-bff6-b79c03a1abec",
   "metadata": {},
   "source": [
    "# Creation of dummy (query,doc) pairs -> Replace by the actual (query, doc) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe685422-11c5-4e50-901f-7e6f774364aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Sample queries\n",
    "query_a = \"Is throat cancer treatable nowadays?\"\n",
    "query_b = \"How to deal with hypothermia?\"\n",
    "\n",
    "# Sample documents\n",
    "sentence_a = \"58-year-old woman with hypertension\"\n",
    "sentence_b = \"BACKGROUND : Longitudinal studies hypertension\"\n",
    "\n",
    "number_documents = 3600\n",
    "\n",
    "# Create dummy (query, document) pairs\n",
    "# TODO: replace by your own (query, document) pairs\n",
    "query_pairs = [(random.choice([query_a, query_b]), \n",
    "           random.choice([sentence_a, sentence_b])) for i in range(number_documents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fa806-a516-4eac-86d7-f039e35d4ec4",
   "metadata": {},
   "source": [
    "## CLS Embedding Extraction in Batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c4240-eb79-4b39-a87f-84a9f3635a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "def extract_cls(query_pairs, embeddings, batch_size=32):\n",
    "\n",
    "    # Iterate over all documents, in batches of size <batch_size>\n",
    "    for batch_idx in range(0, len(query_pairs), batch_size):\n",
    "\n",
    "        # Get the current batch of samples\n",
    "        batch_data = query_pairs[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "        inputs = tokenizer.batch_encode_plus(batch_data, \n",
    "                                       return_tensors='pt',  # pytorch tensors\n",
    "                                       add_special_tokens=True,  # Add CLS and SEP tokens\n",
    "                                       max_length = 512, # Max sequence length\n",
    "                                       truncation = True, # Truncate if sequences exceed the Max Sequence length\n",
    "                                       padding = True) # Add padding to forward sequences with different lengths\n",
    "        \n",
    "        # Forward the batch of (query, doc) sequences\n",
    "        with torch.no_grad():\n",
    "            inputs.to(device)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Get the CLS embeddings for each pair query, document\n",
    "        batch_cls = outputs['hidden_states'][-1][:,0,:]\n",
    "        \n",
    "        # L2-Normalize CLS embeddings. Embeddings norm will be 1.\n",
    "        batch_cls = torch.nn.functional.normalize(batch_cls, p=2, dim=1)\n",
    "        \n",
    "        # Store the extracted CLS embeddings from the batch on the memory-mapped ndarray\n",
    "        embeddings[batch_idx:batch_idx + batch_size] = batch_cls.cpu()\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73969563-e0bc-4304-9611-9a08f8c13fcf",
   "metadata": {},
   "source": [
    "The code below will extract CLS embeddings for all query_pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8676e10d-93a7-4d3a-80df-79dd899936ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy ndarray that will store (in RAM) the CLS embeddings of each (query, doc) pair\n",
    "embeddings = np.zeros((len(query_pairs), 768))\n",
    "\n",
    "embeddings = extract_cls(query_pairs, embeddings=embeddings, batch_size=32)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af13e6-99cf-4256-a553-a2f08c88d4cd",
   "metadata": {},
   "source": [
    "## If you're running into memory issues - Put Numpy Arrays on disk\n",
    "\n",
    "The code below extracts CLS embeddings for all query_pairs and stores them on disk, using persistent Numpy ndarrays. The difference is that they won't be stored on your computer RAM. After creating the array as shown below, the fact that they are on disk is abstracted, and you can use them as you would do with standard numpy arrays.\n",
    "\n",
    "Since it writes to disk, it will be slower than the first option, but the amount of RAM needed will be dramatically reduced.\n",
    "\n",
    "Reference: https://numpy.org/doc/stable/reference/generated/numpy.memmap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e944fa-794f-4022-bb05-f228a7bd9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"cls_embeddings.dat\"\n",
    "\n",
    "# Create a memory-mapped numpy array. The array is stored on disk, not on RAM\n",
    "# The shape argument must match (total number query-doc pairs, CLS embedding size)\n",
    "embeddings_persistent = np.memmap(filename, dtype='float32', mode='w+', shape=(len(query_pairs), 768))\n",
    "\n",
    "embeddings_persistent = extract_cls(query_pairs, embeddings=embeddings_persistent, batch_size=32)\n",
    "print(embeddings_persistent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d89724-b6b1-47c2-8f34-7b3b1ea73fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ri2022",
   "language": "python",
   "name": "ri2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
